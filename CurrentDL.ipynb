{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = pd.read_csv(\"CH2.csv\")\n",
    "ch3 = pd.read_csv(\"CH3.csv\")\n",
    "ch4 = pd.read_csv(\"CH4.csv\")\n",
    "\n",
    "ch2_X = ch2.select_dtypes(exclude = [\"int64\", \"object\"]).copy()\n",
    "ch3_X = ch3.select_dtypes(exclude = [\"int64\", \"object\"]).copy()\n",
    "ch4_X = ch4.select_dtypes(exclude = [\"int64\", \"object\"]).copy()\n",
    "\n",
    "ch2_X.drop([\"0\"], axis = 1, inplace = True)\n",
    "ch3_X.drop([\"0\"], axis = 1, inplace = True)\n",
    "ch4_X.drop([\"0\"], axis = 1, inplace = True)\n",
    "\n",
    "Y = ch2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(X, Y):\n",
    "    fft = X.select_dtypes(\"float64\").T\n",
    "\n",
    "    X = X.drop(np.where(np.max(fft) == np.inf)[0])\n",
    "    Y = Y.drop(np.where(np.max(fft) == np.inf)[0])\n",
    "\n",
    "    Y = Y[~X.isna().any(axis = 1)]\n",
    "    X = X[~X.isna().any(axis = 1)]\n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    Y = Y.reset_index(drop=True)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([ch2_X, ch3_X, ch4_X], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = clean(data, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.reshape(np.linalg.norm(X, axis = 1), (-1,1))\n",
    "X = X / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "      <th>5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2604 rows Ã— 15000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "0     0.000062  0.000085  0.000046  0.000220  0.000061  0.000069  0.000172   \n",
       "1     0.000098  0.000096  0.000105  0.000142  0.000046  0.000115  0.000133   \n",
       "2     0.000134  0.000127  0.000147  0.000177  0.000047  0.000023  0.000065   \n",
       "3     0.000016  0.000111  0.000029  0.000101  0.000062  0.000075  0.000022   \n",
       "4     0.000100  0.000121  0.000050  0.000080  0.000103  0.000018  0.000027   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2599  0.000041  0.000086  0.000066  0.000053  0.000057  0.000071  0.000086   \n",
       "2600  0.000066  0.000130  0.000114  0.000053  0.000074  0.000132  0.000182   \n",
       "2601  0.000165  0.000141  0.000020  0.000047  0.000091  0.000167  0.000031   \n",
       "2602  0.000027  0.000106  0.000044  0.000085  0.000109  0.000052  0.000028   \n",
       "2603  0.000098  0.000031  0.000122  0.000226  0.000030  0.000074  0.000090   \n",
       "\n",
       "             8         9        10  ...      4991      4992      4993  \\\n",
       "0     0.000133  0.000075  0.000140  ...  0.000028  0.000013  0.000028   \n",
       "1     0.000084  0.000007  0.000080  ...  0.000042  0.000024  0.000030   \n",
       "2     0.000154  0.000103  0.000058  ...  0.000043  0.000075  0.000024   \n",
       "3     0.000196  0.000139  0.000088  ...  0.000024  0.000032  0.000016   \n",
       "4     0.000044  0.000035  0.000153  ...  0.000045  0.000049  0.000045   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2599  0.000116  0.000206  0.001093  ...  0.000025  0.000037  0.000077   \n",
       "2600  0.000089  0.000063  0.001823  ...  0.000103  0.000051  0.000032   \n",
       "2601  0.000067  0.000102  0.000776  ...  0.000018  0.000019  0.000083   \n",
       "2602  0.000137  0.000158  0.002249  ...  0.000016  0.000046  0.000019   \n",
       "2603  0.000023  0.000119  0.001611  ...  0.000023  0.000031  0.000038   \n",
       "\n",
       "          4994      4995      4996      4997      4998      4999      5000  \n",
       "0     0.000024  0.000010  0.000041  0.000039  0.000057  0.000006  0.000002  \n",
       "1     0.000036  0.000032  0.000030  0.000045  0.000040  0.000031  0.000054  \n",
       "2     0.000012  0.000023  0.000029  0.000060  0.000048  0.000063  0.000091  \n",
       "3     0.000022  0.000024  0.000012  0.000026  0.000023  0.000040  0.000006  \n",
       "4     0.000038  0.000035  0.000041  0.000013  0.000054  0.000021  0.000079  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2599  0.000006  0.000069  0.000037  0.000021  0.000065  0.000021  0.000007  \n",
       "2600  0.000042  0.000073  0.000035  0.000061  0.000046  0.000028  0.000105  \n",
       "2601  0.000073  0.000041  0.000027  0.000084  0.000105  0.000018  0.000089  \n",
       "2602  0.000048  0.000072  0.000061  0.000025  0.000039  0.000068  0.000129  \n",
       "2603  0.000086  0.000040  0.000057  0.000077  0.000032  0.000065  0.000008  \n",
       "\n",
       "[2604 rows x 15000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "model = Sequential([Input(shape = (15000,)),\n",
    "                    Dense(1000, activation = \"relu\"),\n",
    "                    Dense(200, activation = \"relu\"),\n",
    "                    Dense(50, activation = \"relu\"),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                    Dense(7, activation = \"softmax\")])\n",
    "\n",
    "model.compile(loss = SparseCategoricalCrossentropy(), optimizer = \"rmsprop\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3153 - accuracy: 0.8905\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3044 - accuracy: 0.8833\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3638 - accuracy: 0.8944\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3001 - accuracy: 0.9006\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3206 - accuracy: 0.8905\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3218 - accuracy: 0.8809\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3260 - accuracy: 0.8752\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2986 - accuracy: 0.8997\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2690 - accuracy: 0.9054\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2862 - accuracy: 0.9073\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3115 - accuracy: 0.8963\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2757 - accuracy: 0.9126\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2903 - accuracy: 0.9093\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2987 - accuracy: 0.9030\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2111 - accuracy: 0.9381\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2520 - accuracy: 0.9169\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2468 - accuracy: 0.9217\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2666 - accuracy: 0.9136\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2215 - accuracy: 0.9304\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.2398 - accuracy: 0.9256\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.2792 - accuracy: 0.9189\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2286 - accuracy: 0.9237\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2641 - accuracy: 0.9160\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2236 - accuracy: 0.9299\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - 0s 8ms/step - loss: 0.2138 - accuracy: 0.9333\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2403 - accuracy: 0.9285\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2816 - accuracy: 0.9198\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2523 - accuracy: 0.9280\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2179 - accuracy: 0.9347\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2157 - accuracy: 0.9304\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2460 - accuracy: 0.9342\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.2005 - accuracy: 0.9333\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2731 - accuracy: 0.9150\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.1521 - accuracy: 0.9582\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2121 - accuracy: 0.9333\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.2389 - accuracy: 0.9217\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.2239 - accuracy: 0.9434\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2446 - accuracy: 0.9270\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.1688 - accuracy: 0.9501\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2109 - accuracy: 0.9376\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - 0s 8ms/step - loss: 0.2455 - accuracy: 0.9256\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2030 - accuracy: 0.9410\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.1810 - accuracy: 0.9462\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - 0s 8ms/step - loss: 0.2448 - accuracy: 0.9443\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.1999 - accuracy: 0.9534\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.1225 - accuracy: 0.9645\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2002 - accuracy: 0.9496\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - 0s 8ms/step - loss: 0.2140 - accuracy: 0.9410\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - 0s 8ms/step - loss: 0.2304 - accuracy: 0.9323\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.2000 - accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295e748b888>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
